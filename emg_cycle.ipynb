{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0e070e-a0c2-42b2-b1cd-16f9a37df998",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'EEG_cnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mEEG_cnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mEEG_cnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mEEG_cnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_input\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'EEG_cnn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5931c7-0c99-4154-88a6-f3a572dc210e",
   "metadata": {},
   "source": [
    "### Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b5bc1-f027-4339-b0d0-1e96855e60de",
   "metadata": {},
   "source": [
    "### translating signals to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb06081-fee9-4e3a-8a34-a064a3cf5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycle_gan.models import Discriminator\n",
    "from cycle_gan.models import Generator\n",
    "from cycle_gan.utils import ReplayBuffer\n",
    "from cycle_gan.utils import LambdaLR\n",
    "from cycle_gan.utils import Logger\n",
    "from cycle_gan.utils import weights_init_normal\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aad3019-4d55-4cd0-a5f6-1343c8fffc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG_A2B = Generator(1, 1)\n",
    "netG_B2A = Generator(1,1)\n",
    "netD_A = Discriminator(1)\n",
    "netD_B = Discriminator(1)\n",
    "\n",
    "netG_A2B.cuda()\n",
    "netG_B2A.cuda()\n",
    "netD_A.cuda()\n",
    "netD_B.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc48fc52-68e5-4ceb-b1b5-7b11b2aec914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG_A2B.apply(weights_init_normal)\n",
    "netG_B2A.apply(weights_init_normal)\n",
    "netD_A.apply(weights_init_normal)\n",
    "netD_B.apply(weights_init_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0a414d1-45be-49d9-ae36-eadad9ee4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87ed8ecc-198c-438d-aaea-8ba87c16f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
    "                                lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(200, 0, 100).step)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(200, 0, 100).step)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(200, 0, 100).step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236ca572-327a-4f8c-9c4a-a87a8157a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell everytime that we need to continue the training from a previous run\n",
    "# loading the models\n",
    "netD_A.load_state_dict(torch.load('output_EMG/netD_A.pth'))\n",
    "netD_B.load_state_dict(torch.load('output_EMG/netD_B.pth'))\n",
    "netG_A2B.load_state_dict(torch.load('output_EMG/netG_A2B.pth'))\n",
    "netG_B2A.load_state_dict(torch.load('output_EMG/netG_B2A.pth'))\n",
    "\n",
    "# loading optimizers\n",
    "optimizer_D_A.load_state_dict(torch.load('output_EMG/optimizer_D_A.pth'))\n",
    "optimizer_D_B.load_state_dict(torch.load('output_EMG/optimizer_D_B.pth'))\n",
    "optimizer_G.load_state_dict(torch.load('output_EMG/optimizer_G.pth'))\n",
    "\n",
    "# loading schedulers\n",
    "lr_scheduler_D_A.load_state_dict(torch.load('output_EMG/lr_scheduler_D_A.pth'))\n",
    "lr_scheduler_D_B.load_state_dict(torch.load('output_EMG/lr_scheduler_D_B.pth'))\n",
    "lr_scheduler_G.load_state_dict(torch.load('output_EMG/lr_scheduler_G.pth'))\n",
    "\n",
    "# Mover os modelos para a GPU, se dispon√≠vel\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "netD_A.to(device)\n",
    "netD_B.to(device)\n",
    "netG_A2B.to(device)\n",
    "netG_B2A.to(device)\n",
    "\n",
    "# adapting the optimizers to rhe right device\n",
    "for state in optimizer_D_A.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "for state in optimizer_D_B.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "for state in optimizer_G.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42117099-7e12-4200-a0ea-acd0a111d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "input_A = Tensor(batch_size, 1, 256, 256)\n",
    "input_B = Tensor(batch_size, 1, 256, 256)\n",
    "target_real = Variable(Tensor(batch_size).fill_(1.0), requires_grad=False)\n",
    "target_fake = Variable(Tensor(batch_size).fill_(0.0), requires_grad=False)\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa8a7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToSingleChannel(object):\n",
    "    def __call__(self, tensor):\n",
    "        if tensor.dim() != 1  and tensor.size(0) > 1:\n",
    "            #ignoring the the last dimension from what we saw yesterday\n",
    "            tensor = torch.mean(tensor[:3, :, : ], dim=0, keepdim=True)\n",
    "            #tensor = tensor[0, :, : ].unsqueeze(0)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9f9bade-ca64-436a-a7d4-bcf5875421af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from cycle_gan.datasets import ImageDataset\n",
    "\n",
    "transforms_ = [ transforms.Resize(int(256*1.12), Image.BICUBIC),\n",
    "                transforms.RandomCrop(256), \n",
    "                transforms.ToTensor(),\n",
    "                ConvertToSingleChannel(),\n",
    "                transforms.Normalize((0.5), (0.5))\n",
    "               ]\n",
    "dataloader = DataLoader(ImageDataset('EMG_cyclegan_dataset/', transforms_ = transforms_, unaligned=True), \n",
    "                        batch_size=batch_size, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bdb1ca8-cd97-4b77-84ab-0e113abfe10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c736005-47b2-414e-ad85-c652c1423609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "epochs = 125\n",
    "logger = Logger(epochs, len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511e7a6-1975-456c-bc27-86bb606f0bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filhoij/miniconda3/envs/research-dl1/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/125 [0090/0090] -- loss_G: 6.5592 | loss_G_identity: 1.7513 | loss_G_GAN: 1.2413 | loss_G_cycle: 3.5666 | loss_D: 0.2189 -- ETA: 2:59:54.42985321366810\n",
      "Epoch 002/125 [0090/0090] -- loss_G: 5.5591 | loss_G_identity: 1.3289 | loss_G_GAN: 1.5385 | loss_G_cycle: 2.6918 | loss_D: 0.1211 -- ETA: 2:51:04.951032\n",
      "Epoch 003/125 [0090/0090] -- loss_G: 5.4307 | loss_G_identity: 1.2200 | loss_G_GAN: 1.7280 | loss_G_cycle: 2.4826 | loss_D: 0.0909 -- ETA: 2:47:00.629367\n",
      "Epoch 004/125 [0090/0090] -- loss_G: 5.2818 | loss_G_identity: 1.2244 | loss_G_GAN: 1.5620 | loss_G_cycle: 2.4954 | loss_D: 0.1441 -- ETA: 2:44:18.210136\n",
      "Epoch 005/125 [0090/0090] -- loss_G: 5.5207 | loss_G_identity: 1.2217 | loss_G_GAN: 1.8136 | loss_G_cycle: 2.4854 | loss_D: 0.0488 -- ETA: 2:42:11.113684\n",
      "Epoch 006/125 [0090/0090] -- loss_G: 5.3625 | loss_G_identity: 1.1676 | loss_G_GAN: 1.8130 | loss_G_cycle: 2.3819 | loss_D: 0.0600 -- ETA: 2:40:19.846602\n",
      "Epoch 007/125 [0090/0090] -- loss_G: 5.2423 | loss_G_identity: 1.1698 | loss_G_GAN: 1.6853 | loss_G_cycle: 2.3871 | loss_D: 0.1229 -- ETA: 2:38:33.986964\n",
      "Epoch 008/125 [0090/0090] -- loss_G: 5.3809 | loss_G_identity: 1.1551 | loss_G_GAN: 1.8724 | loss_G_cycle: 2.3534 | loss_D: 0.0195 -- ETA: 2:36:57.447261\n",
      "Epoch 009/125 [0090/0090] -- loss_G: 5.3507 | loss_G_identity: 1.1319 | loss_G_GAN: 1.8899 | loss_G_cycle: 2.3289 | loss_D: 0.0255 -- ETA: 2:35:24.893094\n",
      "Epoch 010/125 [0090/0090] -- loss_G: 5.1495 | loss_G_identity: 1.0932 | loss_G_GAN: 1.8164 | loss_G_cycle: 2.2399 | loss_D: 0.0476 -- ETA: 2:33:55.887325\n",
      "Epoch 011/125 [0090/0090] -- loss_G: 5.2992 | loss_G_identity: 1.1144 | loss_G_GAN: 1.8816 | loss_G_cycle: 2.3032 | loss_D: 0.0324 -- ETA: 2:32:27.585390\n",
      "Epoch 012/125 [0090/0090] -- loss_G: 5.1460 | loss_G_identity: 1.0599 | loss_G_GAN: 1.9012 | loss_G_cycle: 2.1848 | loss_D: 0.0123 -- ETA: 2:31:00.956793\n",
      "Epoch 013/125 [0090/0090] -- loss_G: 5.1347 | loss_G_identity: 1.0316 | loss_G_GAN: 1.9572 | loss_G_cycle: 2.1459 | loss_D: 0.0040 -- ETA: 2:29:35.260300\n",
      "Epoch 014/125 [0090/0090] -- loss_G: 5.1641 | loss_G_identity: 1.0470 | loss_G_GAN: 1.9444 | loss_G_cycle: 2.1728 | loss_D: 0.0066 -- ETA: 2:28:09.681345\n",
      "Epoch 015/125 [0090/0090] -- loss_G: 5.2328 | loss_G_identity: 1.0590 | loss_G_GAN: 1.9216 | loss_G_cycle: 2.2523 | loss_D: 0.0116 -- ETA: 2:26:46.120246\n",
      "Epoch 016/125 [0090/0090] -- loss_G: 4.6595 | loss_G_identity: 1.0456 | loss_G_GAN: 1.4179 | loss_G_cycle: 2.1960 | loss_D: 0.2812 -- ETA: 2:25:22.109092\n",
      "Epoch 017/125 [0090/0090] -- loss_G: 4.4605 | loss_G_identity: 1.0103 | loss_G_GAN: 1.3485 | loss_G_cycle: 2.1018 | loss_D: 0.2286 -- ETA: 2:23:57.801725\n",
      "Epoch 018/125 [0090/0090] -- loss_G: 4.6344 | loss_G_identity: 1.0638 | loss_G_GAN: 1.3658 | loss_G_cycle: 2.2048 | loss_D: 0.1836 -- ETA: 2:22:34.460067\n",
      "Epoch 019/125 [0090/0090] -- loss_G: 4.7154 | loss_G_identity: 1.0469 | loss_G_GAN: 1.4987 | loss_G_cycle: 2.1698 | loss_D: 0.1195 -- ETA: 2:21:12.417604\n",
      "Epoch 020/125 [0090/0090] -- loss_G: 4.8452 | loss_G_identity: 1.0483 | loss_G_GAN: 1.6178 | loss_G_cycle: 2.1791 | loss_D: 0.1544 -- ETA: 2:19:50.359518\n",
      "Epoch 021/125 [0090/0090] -- loss_G: 4.6645 | loss_G_identity: 1.0385 | loss_G_GAN: 1.4745 | loss_G_cycle: 2.1515 | loss_D: 0.1047 -- ETA: 2:18:27.839574\n",
      "Epoch 022/125 [0090/0090] -- loss_G: 4.8107 | loss_G_identity: 1.0199 | loss_G_GAN: 1.6359 | loss_G_cycle: 2.1549 | loss_D: 0.0536 -- ETA: 2:17:06.148001\n",
      "Epoch 023/125 [0090/0090] -- loss_G: 4.8261 | loss_G_identity: 0.9789 | loss_G_GAN: 1.7880 | loss_G_cycle: 2.0592 | loss_D: 0.0341 -- ETA: 2:15:44.551908\n",
      "Epoch 024/125 [0090/0090] -- loss_G: 4.7715 | loss_G_identity: 0.9418 | loss_G_GAN: 1.8004 | loss_G_cycle: 2.0293 | loss_D: 0.0330 -- ETA: 2:14:23.359499\n",
      "Epoch 025/125 [0090/0090] -- loss_G: 4.8281 | loss_G_identity: 0.9412 | loss_G_GAN: 1.8599 | loss_G_cycle: 2.0271 | loss_D: 0.0159 -- ETA: 2:13:01.946972\n",
      "Epoch 026/125 [0090/0090] -- loss_G: 4.7499 | loss_G_identity: 0.9129 | loss_G_GAN: 1.9013 | loss_G_cycle: 1.9357 | loss_D: 0.0121 -- ETA: 2:11:41.044029\n",
      "Epoch 027/125 [0090/0090] -- loss_G: 4.7635 | loss_G_identity: 0.9004 | loss_G_GAN: 1.9121 | loss_G_cycle: 1.9510 | loss_D: 0.0101 -- ETA: 2:10:20.116640\n",
      "Epoch 028/125 [0090/0090] -- loss_G: 4.7501 | loss_G_identity: 0.8883 | loss_G_GAN: 1.9354 | loss_G_cycle: 1.9264 | loss_D: 0.0044 -- ETA: 2:08:59.383653\n",
      "Epoch 029/125 [0090/0090] -- loss_G: 4.8836 | loss_G_identity: 0.9213 | loss_G_GAN: 1.9549 | loss_G_cycle: 2.0074 | loss_D: 0.0024 -- ETA: 2:07:40.220415\n",
      "Epoch 030/125 [0090/0090] -- loss_G: 4.5658 | loss_G_identity: 0.8509 | loss_G_GAN: 1.8360 | loss_G_cycle: 1.8788 | loss_D: 0.0176 -- ETA: 2:06:20.395624\n",
      "Epoch 031/125 [0090/0090] -- loss_G: 4.6323 | loss_G_identity: 0.8565 | loss_G_GAN: 1.9000 | loss_G_cycle: 1.8758 | loss_D: 0.0052 -- ETA: 2:05:00.820553\n",
      "Epoch 032/125 [0090/0090] -- loss_G: 4.5202 | loss_G_identity: 0.8451 | loss_G_GAN: 1.8143 | loss_G_cycle: 1.8608 | loss_D: 0.0375 -- ETA: 2:03:41.096837\n",
      "Epoch 033/125 [0090/0090] -- loss_G: 4.5937 | loss_G_identity: 0.8635 | loss_G_GAN: 1.8566 | loss_G_cycle: 1.8736 | loss_D: 0.0187 -- ETA: 2:02:20.527409\n",
      "Epoch 034/125 [0090/0090] -- loss_G: 4.5049 | loss_G_identity: 0.8170 | loss_G_GAN: 1.9405 | loss_G_cycle: 1.7475 | loss_D: 0.0032 -- ETA: 2:01:00.310730\n",
      "Epoch 035/125 [0090/0090] -- loss_G: 4.5070 | loss_G_identity: 0.8030 | loss_G_GAN: 1.9619 | loss_G_cycle: 1.7422 | loss_D: 0.0018 -- ETA: 1:59:40.263925\n",
      "Epoch 036/125 [0090/0090] -- loss_G: 4.4683 | loss_G_identity: 0.8020 | loss_G_GAN: 1.9470 | loss_G_cycle: 1.7193 | loss_D: 0.0023 -- ETA: 1:58:20.810479\n",
      "Epoch 037/125 [0090/0090] -- loss_G: 4.4303 | loss_G_identity: 0.7784 | loss_G_GAN: 1.9643 | loss_G_cycle: 1.6876 | loss_D: 0.0021 -- ETA: 1:57:00.979442\n",
      "Epoch 038/125 [0090/0090] -- loss_G: 4.4276 | loss_G_identity: 0.7791 | loss_G_GAN: 1.9610 | loss_G_cycle: 1.6875 | loss_D: 0.0054 -- ETA: 1:55:41.481958\n",
      "Epoch 039/125 [0090/0090] -- loss_G: 4.3917 | loss_G_identity: 0.7657 | loss_G_GAN: 1.9337 | loss_G_cycle: 1.6923 | loss_D: 0.0036 -- ETA: 1:54:22.150661\n",
      "Epoch 040/125 [0090/0090] -- loss_G: 4.3162 | loss_G_identity: 0.7410 | loss_G_GAN: 1.9684 | loss_G_cycle: 1.6068 | loss_D: 0.0011 -- ETA: 1:53:02.213224\n",
      "Epoch 041/125 [0090/0090] -- loss_G: 4.4029 | loss_G_identity: 0.7709 | loss_G_GAN: 1.9587 | loss_G_cycle: 1.6733 | loss_D: 0.0033 -- ETA: 1:51:42.223420\n",
      "Epoch 042/125 [0090/0090] -- loss_G: 4.3192 | loss_G_identity: 0.7490 | loss_G_GAN: 1.9521 | loss_G_cycle: 1.6182 | loss_D: 0.0038 -- ETA: 1:50:21.909455\n",
      "Epoch 043/125 [0090/0090] -- loss_G: 4.3088 | loss_G_identity: 0.7480 | loss_G_GAN: 1.9639 | loss_G_cycle: 1.5969 | loss_D: 0.0023 -- ETA: 1:49:01.773867\n",
      "Epoch 044/125 [0090/0090] -- loss_G: 4.2734 | loss_G_identity: 0.7290 | loss_G_GAN: 1.9740 | loss_G_cycle: 1.5705 | loss_D: 0.0009 -- ETA: 1:47:43.254694\n",
      "Epoch 045/125 [0090/0090] -- loss_G: 4.1714 | loss_G_identity: 0.7036 | loss_G_GAN: 1.9815 | loss_G_cycle: 1.4863 | loss_D: 0.0007 -- ETA: 1:46:23.078453\n",
      "Epoch 046/125 [0090/0090] -- loss_G: 4.2274 | loss_G_identity: 0.7191 | loss_G_GAN: 1.9873 | loss_G_cycle: 1.5209 | loss_D: 0.0010 -- ETA: 1:45:02.852149\n",
      "Epoch 047/125 [0090/0090] -- loss_G: 4.1771 | loss_G_identity: 0.7135 | loss_G_GAN: 1.9676 | loss_G_cycle: 1.4960 | loss_D: 0.0022 -- ETA: 1:43:42.841226\n",
      "Epoch 048/125 [0090/0090] -- loss_G: 4.1313 | loss_G_identity: 0.6838 | loss_G_GAN: 1.9852 | loss_G_cycle: 1.4623 | loss_D: 0.0012 -- ETA: 1:42:23.043672\n",
      "Epoch 049/125 [0090/0090] -- loss_G: 4.1780 | loss_G_identity: 0.7024 | loss_G_GAN: 1.9845 | loss_G_cycle: 1.4911 | loss_D: 0.0020 -- ETA: 1:41:03.020012\n",
      "Epoch 050/125 [0090/0090] -- loss_G: 4.1831 | loss_G_identity: 0.6987 | loss_G_GAN: 1.9887 | loss_G_cycle: 1.4957 | loss_D: 0.0011 -- ETA: 1:39:43.274021\n",
      "Epoch 051/125 [0090/0090] -- loss_G: 4.1656 | loss_G_identity: 0.6995 | loss_G_GAN: 1.9877 | loss_G_cycle: 1.4784 | loss_D: 0.0103 -- ETA: 1:38:23.695796\n",
      "Epoch 052/125 [0090/0090] -- loss_G: 3.5423 | loss_G_identity: 0.7195 | loss_G_GAN: 1.2373 | loss_G_cycle: 1.5855 | loss_D: 0.4717 -- ETA: 1:37:04.123489\n",
      "Epoch 053/125 [0090/0090] -- loss_G: 3.8083 | loss_G_identity: 0.7141 | loss_G_GAN: 1.5428 | loss_G_cycle: 1.5513 | loss_D: 0.0824 -- ETA: 1:35:44.507544\n",
      "Epoch 054/125 [0090/0090] -- loss_G: 3.5904 | loss_G_identity: 0.7368 | loss_G_GAN: 1.2851 | loss_G_cycle: 1.5685 | loss_D: 0.1331 -- ETA: 1:34:25.345273\n",
      "Epoch 055/125 [0090/0090] -- loss_G: 3.9529 | loss_G_identity: 0.7267 | loss_G_GAN: 1.6415 | loss_G_cycle: 1.5847 | loss_D: 0.0487 -- ETA: 1:33:05.689507\n",
      "Epoch 056/125 [0090/0090] -- loss_G: 4.0971 | loss_G_identity: 0.7074 | loss_G_GAN: 1.8095 | loss_G_cycle: 1.5801 | loss_D: 0.0165 -- ETA: 1:31:46.148517\n",
      "Epoch 057/125 [0090/0090] -- loss_G: 4.1349 | loss_G_identity: 0.7208 | loss_G_GAN: 1.8286 | loss_G_cycle: 1.5856 | loss_D: 0.0181 -- ETA: 1:30:26.744727\n",
      "Epoch 058/125 [0090/0090] -- loss_G: 4.0250 | loss_G_identity: 0.6829 | loss_G_GAN: 1.8730 | loss_G_cycle: 1.4691 | loss_D: 0.0170 -- ETA: 1:29:06.908707\n",
      "Epoch 059/125 [0090/0090] -- loss_G: 4.0357 | loss_G_identity: 0.6684 | loss_G_GAN: 1.8911 | loss_G_cycle: 1.4761 | loss_D: 0.0096 -- ETA: 1:27:47.363111\n",
      "Epoch 060/125 [0090/0090] -- loss_G: 4.0518 | loss_G_identity: 0.6806 | loss_G_GAN: 1.8808 | loss_G_cycle: 1.4904 | loss_D: 0.0141 -- ETA: 1:26:27.712674\n",
      "Epoch 061/125 [0090/0090] -- loss_G: 4.0742 | loss_G_identity: 0.6785 | loss_G_GAN: 1.9184 | loss_G_cycle: 1.4774 | loss_D: 0.0099 -- ETA: 1:25:07.945370\n",
      "Epoch 062/125 [0090/0090] -- loss_G: 4.0728 | loss_G_identity: 0.6643 | loss_G_GAN: 1.9391 | loss_G_cycle: 1.4694 | loss_D: 0.0110 -- ETA: 1:23:48.323927\n",
      "Epoch 063/125 [0090/0090] -- loss_G: 3.9379 | loss_G_identity: 0.6572 | loss_G_GAN: 1.8779 | loss_G_cycle: 1.4028 | loss_D: 0.0170 -- ETA: 1:22:28.421222\n",
      "Epoch 064/125 [0090/0090] -- loss_G: 3.9716 | loss_G_identity: 0.6544 | loss_G_GAN: 1.9138 | loss_G_cycle: 1.4034 | loss_D: 0.0091 -- ETA: 1:21:08.510201\n",
      "Epoch 065/125 [0090/0090] -- loss_G: 4.0568 | loss_G_identity: 0.6691 | loss_G_GAN: 1.9316 | loss_G_cycle: 1.4560 | loss_D: 0.0067 -- ETA: 1:19:48.591966\n",
      "Epoch 066/125 [0090/0090] -- loss_G: 3.9537 | loss_G_identity: 0.6326 | loss_G_GAN: 1.9519 | loss_G_cycle: 1.3692 | loss_D: 0.0081 -- ETA: 1:18:28.564929\n",
      "Epoch 067/125 [0090/0090] -- loss_G: 3.9140 | loss_G_identity: 0.6598 | loss_G_GAN: 1.8400 | loss_G_cycle: 1.4143 | loss_D: 0.0277 -- ETA: 1:17:08.749398\n",
      "Epoch 068/125 [0090/0090] -- loss_G: 3.8934 | loss_G_identity: 0.6209 | loss_G_GAN: 1.9024 | loss_G_cycle: 1.3700 | loss_D: 0.0075 -- ETA: 1:15:49.048619\n",
      "Epoch 069/125 [0090/0090] -- loss_G: 3.8950 | loss_G_identity: 0.6252 | loss_G_GAN: 1.9017 | loss_G_cycle: 1.3681 | loss_D: 0.0216 -- ETA: 1:14:29.332808\n",
      "Epoch 070/125 [0090/0090] -- loss_G: 3.8457 | loss_G_identity: 0.6222 | loss_G_GAN: 1.8789 | loss_G_cycle: 1.3446 | loss_D: 0.0089 -- ETA: 1:13:09.502066\n",
      "Epoch 071/125 [0090/0090] -- loss_G: 4.0050 | loss_G_identity: 0.6545 | loss_G_GAN: 1.9322 | loss_G_cycle: 1.4183 | loss_D: 0.0082 -- ETA: 1:11:49.526690\n",
      "Epoch 072/125 [0090/0090] -- loss_G: 3.8933 | loss_G_identity: 0.6326 | loss_G_GAN: 1.8855 | loss_G_cycle: 1.3752 | loss_D: 0.0172 -- ETA: 1:10:29.639367\n",
      "Epoch 073/125 [0090/0090] -- loss_G: 3.7800 | loss_G_identity: 0.6254 | loss_G_GAN: 1.8241 | loss_G_cycle: 1.3305 | loss_D: 0.0185 -- ETA: 1:09:09.887230\n",
      "Epoch 074/125 [0090/0090] -- loss_G: 3.7394 | loss_G_identity: 0.6103 | loss_G_GAN: 1.8219 | loss_G_cycle: 1.3072 | loss_D: 0.0314 -- ETA: 1:07:49.912799\n",
      "Epoch 075/125 [0090/0090] -- loss_G: 3.8250 | loss_G_identity: 0.6019 | loss_G_GAN: 1.9113 | loss_G_cycle: 1.3119 | loss_D: 0.0069 -- ETA: 1:06:30.060719\n",
      "Epoch 076/125 [0090/0090] -- loss_G: 3.8753 | loss_G_identity: 0.6162 | loss_G_GAN: 1.9451 | loss_G_cycle: 1.3140 | loss_D: 0.0065 -- ETA: 1:05:10.382788\n",
      "Epoch 077/125 [0090/0090] -- loss_G: 3.7904 | loss_G_identity: 0.5831 | loss_G_GAN: 1.9467 | loss_G_cycle: 1.2607 | loss_D: 0.0039 -- ETA: 1:03:50.576098\n",
      "Epoch 078/125 [0090/0090] -- loss_G: 3.7818 | loss_G_identity: 0.5815 | loss_G_GAN: 1.9482 | loss_G_cycle: 1.2521 | loss_D: 0.0042 -- ETA: 1:02:30.715898\n",
      "Epoch 079/125 [0017/0090] -- loss_G: 3.8141 | loss_G_identity: 0.5914 | loss_G_GAN: 1.9026 | loss_G_cycle: 1.3201 | loss_D: 0.0096 -- ETA: 1:02:16.427136"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Set model input\n",
    "        real_A = Variable(input_A.copy_(batch['A']))\n",
    "        real_B = Variable(input_B.copy_(batch['B']))\n",
    "\n",
    "        ###### Generators A2B and B2A ######\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        # G_A2B(B) should equal B if real B is fed\n",
    "        same_B = netG_A2B(real_B)\n",
    "        loss_identity_B = criterion_identity(same_B, real_B)*5.0\n",
    "        # G_B2A(A) should equal A if real A is fed\n",
    "        same_A = netG_B2A(real_A)\n",
    "        loss_identity_A = criterion_identity(same_A, real_A)*5.0\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = netG_A2B(real_A)\n",
    "        pred_fake = netD_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, target_real)\n",
    "\n",
    "        fake_A = netG_B2A(real_B)\n",
    "        pred_fake = netD_A(fake_A)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake, target_real)\n",
    "\n",
    "        # Cycle loss\n",
    "        recovered_A = netG_B2A(fake_B)\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A)*10.0\n",
    "\n",
    "        recovered_B = netG_A2B(fake_A)\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B)*10.0\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "        loss_G.backward()\n",
    "        \n",
    "        optimizer_G.step()\n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator A ######\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = netD_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        fake_A = fake_A_buffer.push_and_pop(fake_A)\n",
    "        pred_fake = netD_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_A.backward()\n",
    "\n",
    "        optimizer_D_A.step()\n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator B ######\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = netD_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "        \n",
    "        # Fake loss\n",
    "        fake_B = fake_B_buffer.push_and_pop(fake_B)\n",
    "        pred_fake = netD_B(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_B.backward()\n",
    "\n",
    "        optimizer_D_B.step()\n",
    "        ###################################\n",
    "\n",
    "        # Progress report (http://localhost:8097)\n",
    "        logger.log({'loss_G': loss_G, 'loss_G_identity': (loss_identity_A + loss_identity_B), 'loss_G_GAN': (loss_GAN_A2B + loss_GAN_B2A),\n",
    "                    'loss_G_cycle': (loss_cycle_ABA + loss_cycle_BAB), 'loss_D': (loss_D_A + loss_D_B)}, \n",
    "                    images={'real_A': real_A, 'real_B': real_B, 'fake_A': fake_A, 'fake_B': fake_B})\n",
    "    \n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    # Save models checkpoints\n",
    "    torch.save(netG_A2B.state_dict(), 'output_EMG/netG_A2B.pth')\n",
    "    torch.save(netG_B2A.state_dict(), 'output_EMG/netG_B2A.pth')\n",
    "    torch.save(netD_A.state_dict(), 'output_EMG/netD_A.pth')\n",
    "    torch.save(netD_B.state_dict(), 'output_EMG/netD_B.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15c2fe-5814-4a3e-a1bd-3ddf49ab3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(optimizer_G.state_dict(), 'output_EMG/optimizer_G.pth')\n",
    "torch.save(optimizer_D_A.state_dict(), 'output_EMG/optimizer_D_A.pth')\n",
    "torch.save(optimizer_D_B.state_dict(), 'output_EMG/optimizer_D_B.pth')\n",
    "\n",
    "#learningrateschedulers\n",
    "\n",
    "torch.save(lr_scheduler_G.state_dict(), 'output_EMG/lr_scheduler_G.pth')\n",
    "torch.save(lr_scheduler_D_A.state_dict(), 'output_EMG/lr_scheduler_D_A.pth')\n",
    "torch.save(lr_scheduler_D_B.state_dict(), 'output_EMG/lr_scheduler_D_B.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4032d-9e74-45aa-90c9-375186851732",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G.load_state_dict(torch.load('output_EMG/optimizer_G.pth'))\n",
    "optimizer_D_A.load_state_dict(torch.load('output_EMG/optimizer_D_A.pth'))\n",
    "optimizer_D_B.load_state_dict(torch.load('output_EMG/optimizer_D_B.pth'))\n",
    "\n",
    "#learningrateschedulers\n",
    "\n",
    "lr_scheduler_G.load_state_dict(torch.load('output_EMG/lr_scheduler_G.pth'))\n",
    "lr_scheduler_D_A.load_state_dict(torch.load('output_EMG/lr_scheduler_D_A.pth'))\n",
    "lr_scheduler_D_B.load_state_dict(torch.load('output_EMG/lr_scheduler_D_B.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a12d4-9078-4a00-af8d-65fde496851c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
