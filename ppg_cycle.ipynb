{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0e070e-a0c2-42b2-b1cd-16f9a37df998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5931c7-0c99-4154-88a6-f3a572dc210e",
   "metadata": {},
   "source": [
    "### Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b5bc1-f027-4339-b0d0-1e96855e60de",
   "metadata": {},
   "source": [
    "### translating signals to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb06081-fee9-4e3a-8a34-a064a3cf5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycle_gan.models import Discriminator\n",
    "from cycle_gan.models import Generator\n",
    "from cycle_gan.utils import ReplayBuffer\n",
    "from cycle_gan.utils import LambdaLR\n",
    "from cycle_gan.utils import Logger\n",
    "from cycle_gan.utils import weights_init_normal\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aad3019-4d55-4cd0-a5f6-1343c8fffc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG_A2B = Generator(1, 1)\n",
    "netG_B2A = Generator(1,1)\n",
    "netD_A = Discriminator(1)\n",
    "netD_B = Discriminator(1)\n",
    "\n",
    "netG_A2B.cuda()\n",
    "netG_B2A.cuda()\n",
    "netD_A.cuda()\n",
    "netD_B.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc48fc52-68e5-4ceb-b1b5-7b11b2aec914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG_A2B.apply(weights_init_normal)\n",
    "netG_B2A.apply(weights_init_normal)\n",
    "netD_A.apply(weights_init_normal)\n",
    "netD_B.apply(weights_init_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a414d1-45be-49d9-ae36-eadad9ee4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ed8ecc-198c-438d-aaea-8ba87c16f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
    "                                lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(200, 0, 100).step)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(200, 0, 100).step)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(200, 0, 100).step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236ca572-327a-4f8c-9c4a-a87a8157a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell everytime that we need to continue the training from a previous run\n",
    "# loading the models\n",
    "netD_A.load_state_dict(torch.load('output_PPG/netD_A.pth'))\n",
    "netD_B.load_state_dict(torch.load('output_PPG/netD_B.pth'))\n",
    "netG_A2B.load_state_dict(torch.load('output_PPG/netG_A2B.pth'))\n",
    "netG_B2A.load_state_dict(torch.load('output_PPG/netG_B2A.pth'))\n",
    "\n",
    "# loading optimizers\n",
    "optimizer_D_A.load_state_dict(torch.load('output_PPG/optimizer_D_A.pth'))\n",
    "optimizer_D_B.load_state_dict(torch.load('output_PPG/optimizer_D_B.pth'))\n",
    "optimizer_G.load_state_dict(torch.load('output_PPG/optimizer_G.pth'))\n",
    "\n",
    "# loading schedulers\n",
    "lr_scheduler_D_A.load_state_dict(torch.load('output_PPG/lr_scheduler_D_A.pth'))\n",
    "lr_scheduler_D_B.load_state_dict(torch.load('output_PPG/lr_scheduler_D_B.pth'))\n",
    "lr_scheduler_G.load_state_dict(torch.load('output_PPG/lr_scheduler_G.pth'))\n",
    "\n",
    "# Mover os modelos para a GPU, se disponÃ­vel\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "netD_A.to(device)\n",
    "netD_B.to(device)\n",
    "netG_A2B.to(device)\n",
    "netG_B2A.to(device)\n",
    "\n",
    "# adapting the optimizers to rhe right device\n",
    "for state in optimizer_D_A.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "for state in optimizer_D_B.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "for state in optimizer_G.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42117099-7e12-4200-a0ea-acd0a111d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "input_A = Tensor(batch_size, 1, 256, 256)\n",
    "input_B = Tensor(batch_size, 1, 256, 256)\n",
    "target_real = Variable(Tensor(batch_size).fill_(1.0), requires_grad=False)\n",
    "target_fake = Variable(Tensor(batch_size).fill_(0.0), requires_grad=False)\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8a7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToSingleChannel(object):\n",
    "    def __call__(self, tensor):\n",
    "        if tensor.dim() != 1  and tensor.size(0) > 1:\n",
    "            #ignoring the the last dimension from what we saw yesterday\n",
    "            tensor = torch.mean(tensor[:3, :, : ], dim=0, keepdim=True)\n",
    "            #tensor = tensor[0, :, : ].unsqueeze(0)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9f9bade-ca64-436a-a7d4-bcf5875421af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filhoij/miniconda3/envs/research-dl1/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from cycle_gan.datasets import ImageDataset\n",
    "\n",
    "transforms_ = [ transforms.Resize(int(256*1.12), Image.BICUBIC),\n",
    "                transforms.RandomCrop(256), \n",
    "                transforms.ToTensor(),\n",
    "                ConvertToSingleChannel(),\n",
    "                transforms.Normalize((0.5), (0.5))\n",
    "               ]\n",
    "dataloader = DataLoader(ImageDataset('PPG_cyclegan_dataset/', transforms_ = transforms_, unaligned=True), \n",
    "                        batch_size=batch_size, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bdb1ca8-cd97-4b77-84ab-0e113abfe10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c736005-47b2-414e-ad85-c652c1423609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "logger = Logger(epochs, len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511e7a6-1975-456c-bc27-86bb606f0bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filhoij/miniconda3/envs/research-dl1/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/200 [0100/0100] -- loss_G: 5.7069 | loss_G_identity: 1.4932 | loss_G_GAN: 0.8730 | loss_G_cycle: 3.3407 | loss_D: 0.3849 -- ETA: 10:41:21.0456934668327333\n",
      "Epoch 002/200 [0100/0100] -- loss_G: 4.4902 | loss_G_identity: 1.0144 | loss_G_GAN: 1.0849 | loss_G_cycle: 2.3909 | loss_D: 0.2463 -- ETA: 9:49:34.7209452\n",
      "Epoch 003/200 [0100/0100] -- loss_G: 4.1708 | loss_G_identity: 0.8916 | loss_G_GAN: 1.2154 | loss_G_cycle: 2.0638 | loss_D: 0.2361 -- ETA: 9:30:15.644375\n",
      "Epoch 004/200 [0100/0100] -- loss_G: 4.0479 | loss_G_identity: 0.8281 | loss_G_GAN: 1.2539 | loss_G_cycle: 1.9660 | loss_D: 0.2213 -- ETA: 9:19:40.143512\n",
      "Epoch 005/200 [0100/0100] -- loss_G: 4.0151 | loss_G_identity: 0.7925 | loss_G_GAN: 1.3780 | loss_G_cycle: 1.8446 | loss_D: 0.1911 -- ETA: 9:11:47.068957\n",
      "Epoch 006/200 [0100/0100] -- loss_G: 3.4592 | loss_G_identity: 0.7461 | loss_G_GAN: 0.9968 | loss_G_cycle: 1.7163 | loss_D: 0.2891 -- ETA: 9:05:41.079281\n",
      "Epoch 007/200 [0100/0100] -- loss_G: 3.8194 | loss_G_identity: 0.7689 | loss_G_GAN: 1.2519 | loss_G_cycle: 1.7985 | loss_D: 0.2376 -- ETA: 9:00:30.892522\n",
      "Epoch 008/200 [0100/0100] -- loss_G: 3.4075 | loss_G_identity: 0.7164 | loss_G_GAN: 1.0361 | loss_G_cycle: 1.6551 | loss_D: 0.3092 -- ETA: 8:55:47.920847\n",
      "Epoch 009/200 [0100/0100] -- loss_G: 3.4351 | loss_G_identity: 0.7065 | loss_G_GAN: 1.0832 | loss_G_cycle: 1.6454 | loss_D: 0.2901 -- ETA: 8:51:33.854719\n",
      "Epoch 010/200 [0100/0100] -- loss_G: 3.4129 | loss_G_identity: 0.7034 | loss_G_GAN: 1.0635 | loss_G_cycle: 1.6460 | loss_D: 0.2981 -- ETA: 8:47:40.723873\n",
      "Epoch 011/200 [0100/0100] -- loss_G: 3.4663 | loss_G_identity: 0.7127 | loss_G_GAN: 1.0710 | loss_G_cycle: 1.6826 | loss_D: 0.2888 -- ETA: 8:43:59.469801\n",
      "Epoch 012/200 [0100/0100] -- loss_G: 3.7499 | loss_G_identity: 0.7339 | loss_G_GAN: 1.3071 | loss_G_cycle: 1.7089 | loss_D: 0.2129 -- ETA: 8:40:26.785120\n",
      "Epoch 013/200 [0100/0100] -- loss_G: 3.2749 | loss_G_identity: 0.7028 | loss_G_GAN: 0.9521 | loss_G_cycle: 1.6201 | loss_D: 0.3611 -- ETA: 8:37:01.400700\n",
      "Epoch 014/200 [0100/0100] -- loss_G: 2.7663 | loss_G_identity: 0.6303 | loss_G_GAN: 0.7081 | loss_G_cycle: 1.4280 | loss_D: 0.3906 -- ETA: 8:33:40.867262\n",
      "Epoch 015/200 [0100/0100] -- loss_G: 3.0623 | loss_G_identity: 0.6655 | loss_G_GAN: 0.8493 | loss_G_cycle: 1.5475 | loss_D: 0.3710 -- ETA: 8:30:25.564639\n",
      "Epoch 016/200 [0100/0100] -- loss_G: 3.0157 | loss_G_identity: 0.6522 | loss_G_GAN: 0.8618 | loss_G_cycle: 1.5018 | loss_D: 0.3464 -- ETA: 8:27:16.823461\n",
      "Epoch 017/200 [0100/0100] -- loss_G: 2.8217 | loss_G_identity: 0.6259 | loss_G_GAN: 0.8108 | loss_G_cycle: 1.3850 | loss_D: 0.3724 -- ETA: 8:24:07.890096\n",
      "Epoch 018/200 [0100/0100] -- loss_G: 2.7422 | loss_G_identity: 0.6114 | loss_G_GAN: 0.7825 | loss_G_cycle: 1.3483 | loss_D: 0.3927 -- ETA: 8:21:03.061135\n",
      "Epoch 019/200 [0100/0100] -- loss_G: 2.7968 | loss_G_identity: 0.6176 | loss_G_GAN: 0.7964 | loss_G_cycle: 1.3828 | loss_D: 0.4086 -- ETA: 8:17:59.156576\n",
      "Epoch 020/200 [0100/0100] -- loss_G: 2.6504 | loss_G_identity: 0.6040 | loss_G_GAN: 0.7255 | loss_G_cycle: 1.3209 | loss_D: 0.3962 -- ETA: 8:14:57.010584\n",
      "Epoch 021/200 [0100/0100] -- loss_G: 2.6748 | loss_G_identity: 0.5967 | loss_G_GAN: 0.7868 | loss_G_cycle: 1.2914 | loss_D: 0.3832 -- ETA: 8:11:59.968685\n",
      "Epoch 022/200 [0100/0100] -- loss_G: 2.7188 | loss_G_identity: 0.6220 | loss_G_GAN: 0.7629 | loss_G_cycle: 1.3339 | loss_D: 0.3886 -- ETA: 8:09:00.139377\n",
      "Epoch 023/200 [0100/0100] -- loss_G: 2.6973 | loss_G_identity: 0.6122 | loss_G_GAN: 0.7669 | loss_G_cycle: 1.3182 | loss_D: 0.4176 -- ETA: 8:06:02.652241\n",
      "Epoch 024/200 [0100/0100] -- loss_G: 2.7062 | loss_G_identity: 0.6070 | loss_G_GAN: 0.8019 | loss_G_cycle: 1.2972 | loss_D: 0.4050 -- ETA: 8:03:06.460392\n",
      "Epoch 025/200 [0100/0100] -- loss_G: 2.7297 | loss_G_identity: 0.6178 | loss_G_GAN: 0.7796 | loss_G_cycle: 1.3323 | loss_D: 0.4019 -- ETA: 8:00:10.463334\n",
      "Epoch 026/200 [0100/0100] -- loss_G: 2.8715 | loss_G_identity: 0.6422 | loss_G_GAN: 0.8694 | loss_G_cycle: 1.3599 | loss_D: 0.3485 -- ETA: 7:57:28.632612\n",
      "Epoch 027/200 [0100/0100] -- loss_G: 2.7260 | loss_G_identity: 0.6088 | loss_G_GAN: 0.8306 | loss_G_cycle: 1.2865 | loss_D: 0.3879 -- ETA: 7:54:34.409649\n",
      "Epoch 028/200 [0100/0100] -- loss_G: 2.5991 | loss_G_identity: 0.5753 | loss_G_GAN: 0.7974 | loss_G_cycle: 1.2263 | loss_D: 0.3767 -- ETA: 7:51:41.214226\n",
      "Epoch 029/200 [0100/0100] -- loss_G: 2.7492 | loss_G_identity: 0.5931 | loss_G_GAN: 0.8891 | loss_G_cycle: 1.2670 | loss_D: 0.3648 -- ETA: 7:48:49.999644\n",
      "Epoch 030/200 [0100/0100] -- loss_G: 2.7863 | loss_G_identity: 0.6067 | loss_G_GAN: 0.8721 | loss_G_cycle: 1.3075 | loss_D: 0.3622 -- ETA: 7:45:58.584109\n",
      "Epoch 031/200 [0100/0100] -- loss_G: 2.7425 | loss_G_identity: 0.5960 | loss_G_GAN: 0.8583 | loss_G_cycle: 1.2881 | loss_D: 0.3562 -- ETA: 7:43:08.358807\n",
      "Epoch 032/200 [0100/0100] -- loss_G: 2.6224 | loss_G_identity: 0.5729 | loss_G_GAN: 0.8470 | loss_G_cycle: 1.2025 | loss_D: 0.3735 -- ETA: 7:40:16.891379\n",
      "Epoch 033/200 [0100/0100] -- loss_G: 2.6541 | loss_G_identity: 0.5835 | loss_G_GAN: 0.8527 | loss_G_cycle: 1.2179 | loss_D: 0.3688 -- ETA: 7:37:27.178708\n",
      "Epoch 034/200 [0100/0100] -- loss_G: 2.6296 | loss_G_identity: 0.5784 | loss_G_GAN: 0.8711 | loss_G_cycle: 1.1800 | loss_D: 0.3584 -- ETA: 7:34:37.470489\n",
      "Epoch 035/200 [0100/0100] -- loss_G: 2.5621 | loss_G_identity: 0.5544 | loss_G_GAN: 0.8489 | loss_G_cycle: 1.1589 | loss_D: 0.3641 -- ETA: 7:31:47.816596\n",
      "Epoch 036/200 [0100/0100] -- loss_G: 2.6511 | loss_G_identity: 0.5806 | loss_G_GAN: 0.8629 | loss_G_cycle: 1.2076 | loss_D: 0.3610 -- ETA: 7:28:59.238157\n",
      "Epoch 037/200 [0100/0100] -- loss_G: 2.5080 | loss_G_identity: 0.5416 | loss_G_GAN: 0.8596 | loss_G_cycle: 1.1068 | loss_D: 0.3690 -- ETA: 7:26:11.069834\n",
      "Epoch 038/200 [0100/0100] -- loss_G: 2.5284 | loss_G_identity: 0.5465 | loss_G_GAN: 0.8612 | loss_G_cycle: 1.1207 | loss_D: 0.3561 -- ETA: 7:23:23.568684\n",
      "Epoch 039/200 [0100/0100] -- loss_G: 2.5585 | loss_G_identity: 0.5653 | loss_G_GAN: 0.8377 | loss_G_cycle: 1.1555 | loss_D: 0.3602 -- ETA: 7:20:35.538693\n",
      "Epoch 040/200 [0100/0100] -- loss_G: 2.5246 | loss_G_identity: 0.5369 | loss_G_GAN: 0.8921 | loss_G_cycle: 1.0955 | loss_D: 0.3520 -- ETA: 7:17:47.326865\n",
      "Epoch 041/200 [0100/0100] -- loss_G: 2.7036 | loss_G_identity: 0.5845 | loss_G_GAN: 0.9015 | loss_G_cycle: 1.2177 | loss_D: 0.3494 -- ETA: 7:14:59.676086\n",
      "Epoch 042/200 [0100/0100] -- loss_G: 2.4722 | loss_G_identity: 0.5335 | loss_G_GAN: 0.8412 | loss_G_cycle: 1.0974 | loss_D: 0.3704 -- ETA: 7:12:12.875049\n",
      "Epoch 043/200 [0100/0100] -- loss_G: 2.6873 | loss_G_identity: 0.5864 | loss_G_GAN: 0.8918 | loss_G_cycle: 1.2092 | loss_D: 0.3502 -- ETA: 7:09:25.664150\n",
      "Epoch 044/200 [0100/0100] -- loss_G: 2.5843 | loss_G_identity: 0.5605 | loss_G_GAN: 0.8705 | loss_G_cycle: 1.1532 | loss_D: 0.3582 -- ETA: 7:06:40.481629\n",
      "Epoch 045/200 [0100/0100] -- loss_G: 2.5737 | loss_G_identity: 0.5543 | loss_G_GAN: 0.9003 | loss_G_cycle: 1.1191 | loss_D: 0.3398 -- ETA: 7:03:53.705253\n",
      "Epoch 046/200 [0100/0100] -- loss_G: 2.6157 | loss_G_identity: 0.5657 | loss_G_GAN: 0.8936 | loss_G_cycle: 1.1565 | loss_D: 0.3602 -- ETA: 7:01:07.184043\n",
      "Epoch 047/200 [0100/0100] -- loss_G: 2.6073 | loss_G_identity: 0.5569 | loss_G_GAN: 0.9235 | loss_G_cycle: 1.1269 | loss_D: 0.3327 -- ETA: 6:58:20.369199\n",
      "Epoch 048/200 [0100/0100] -- loss_G: 2.8289 | loss_G_identity: 0.5980 | loss_G_GAN: 0.9503 | loss_G_cycle: 1.2806 | loss_D: 0.3274 -- ETA: 6:55:33.903337\n",
      "Epoch 049/200 [0100/0100] -- loss_G: 2.6602 | loss_G_identity: 0.5560 | loss_G_GAN: 0.9477 | loss_G_cycle: 1.1564 | loss_D: 0.3108 -- ETA: 6:52:47.694892\n",
      "Epoch 050/200 [0100/0100] -- loss_G: 2.6191 | loss_G_identity: 0.5526 | loss_G_GAN: 0.9366 | loss_G_cycle: 1.1299 | loss_D: 0.3166 -- ETA: 6:50:01.477873\n",
      "Epoch 051/200 [0100/0100] -- loss_G: 2.5730 | loss_G_identity: 0.5438 | loss_G_GAN: 0.9379 | loss_G_cycle: 1.0914 | loss_D: 0.3431 -- ETA: 6:47:15.661224\n",
      "Epoch 052/200 [0100/0100] -- loss_G: 2.5528 | loss_G_identity: 0.5320 | loss_G_GAN: 0.9626 | loss_G_cycle: 1.0582 | loss_D: 0.3350 -- ETA: 6:44:29.765408\n",
      "Epoch 053/200 [0100/0100] -- loss_G: 2.6310 | loss_G_identity: 0.5475 | loss_G_GAN: 0.9549 | loss_G_cycle: 1.1286 | loss_D: 0.3192 -- ETA: 6:41:43.665331\n",
      "Epoch 054/200 [0100/0100] -- loss_G: 2.5620 | loss_G_identity: 0.5248 | loss_G_GAN: 0.9829 | loss_G_cycle: 1.0542 | loss_D: 0.3137 -- ETA: 6:38:58.343622\n",
      "Epoch 055/200 [0100/0100] -- loss_G: 2.6319 | loss_G_identity: 0.5531 | loss_G_GAN: 0.9604 | loss_G_cycle: 1.1184 | loss_D: 0.3241 -- ETA: 6:36:12.362285\n",
      "Epoch 056/200 [0100/0100] -- loss_G: 2.5064 | loss_G_identity: 0.5215 | loss_G_GAN: 0.9542 | loss_G_cycle: 1.0307 | loss_D: 0.3317 -- ETA: 6:33:27.115402\n",
      "Epoch 057/200 [0100/0100] -- loss_G: 2.6234 | loss_G_identity: 0.5517 | loss_G_GAN: 0.9651 | loss_G_cycle: 1.1065 | loss_D: 0.3167 -- ETA: 6:30:41.670398\n",
      "Epoch 058/200 [0100/0100] -- loss_G: 2.5216 | loss_G_identity: 0.5242 | loss_G_GAN: 0.9794 | loss_G_cycle: 1.0180 | loss_D: 0.3195 -- ETA: 6:27:56.307639\n",
      "Epoch 059/200 [0100/0100] -- loss_G: 2.5586 | loss_G_identity: 0.5332 | loss_G_GAN: 0.9550 | loss_G_cycle: 1.0704 | loss_D: 0.3176 -- ETA: 6:25:11.020445\n",
      "Epoch 060/200 [0100/0100] -- loss_G: 2.5114 | loss_G_identity: 0.5167 | loss_G_GAN: 0.9854 | loss_G_cycle: 1.0094 | loss_D: 0.3255 -- ETA: 6:22:25.526276\n",
      "Epoch 061/200 [0100/0100] -- loss_G: 2.4422 | loss_G_identity: 0.5014 | loss_G_GAN: 0.9667 | loss_G_cycle: 0.9741 | loss_D: 0.3133 -- ETA: 6:19:40.465387\n",
      "Epoch 062/200 [0100/0100] -- loss_G: 2.9218 | loss_G_identity: 0.5931 | loss_G_GAN: 1.0698 | loss_G_cycle: 1.2588 | loss_D: 0.3010 -- ETA: 6:16:55.386689\n",
      "Epoch 063/200 [0100/0100] -- loss_G: 2.3922 | loss_G_identity: 0.5147 | loss_G_GAN: 0.8497 | loss_G_cycle: 1.0278 | loss_D: 0.3997 -- ETA: 6:14:10.440288\n",
      "Epoch 064/200 [0100/0100] -- loss_G: 2.3795 | loss_G_identity: 0.5111 | loss_G_GAN: 0.8629 | loss_G_cycle: 1.0055 | loss_D: 0.3522 -- ETA: 6:11:25.157851\n",
      "Epoch 065/200 [0100/0100] -- loss_G: 2.4784 | loss_G_identity: 0.5171 | loss_G_GAN: 0.9646 | loss_G_cycle: 0.9967 | loss_D: 0.3129 -- ETA: 6:08:39.944574\n",
      "Epoch 066/200 [0100/0100] -- loss_G: 2.4940 | loss_G_identity: 0.5155 | loss_G_GAN: 0.9884 | loss_G_cycle: 0.9901 | loss_D: 0.3096 -- ETA: 6:05:55.007737\n",
      "Epoch 067/200 [0100/0100] -- loss_G: 2.4996 | loss_G_identity: 0.5192 | loss_G_GAN: 0.9902 | loss_G_cycle: 0.9902 | loss_D: 0.3094 -- ETA: 6:03:10.685505\n",
      "Epoch 068/200 [0100/0100] -- loss_G: 2.5925 | loss_G_identity: 0.5263 | loss_G_GAN: 1.0374 | loss_G_cycle: 1.0287 | loss_D: 0.2920 -- ETA: 6:00:26.765705\n",
      "Epoch 069/200 [0100/0100] -- loss_G: 2.4901 | loss_G_identity: 0.5150 | loss_G_GAN: 0.9969 | loss_G_cycle: 0.9783 | loss_D: 0.3015 -- ETA: 5:57:42.022103\n",
      "Epoch 070/200 [0100/0100] -- loss_G: 2.5269 | loss_G_identity: 0.5240 | loss_G_GAN: 0.9891 | loss_G_cycle: 1.0138 | loss_D: 0.3193 -- ETA: 5:54:57.256967\n",
      "Epoch 071/200 [0100/0100] -- loss_G: 2.5435 | loss_G_identity: 0.5331 | loss_G_GAN: 1.0076 | loss_G_cycle: 1.0028 | loss_D: 0.2893 -- ETA: 5:52:12.630081\n",
      "Epoch 073/200 [0100/0100] -- loss_G: 2.5426 | loss_G_identity: 0.5080 | loss_G_GAN: 1.0564 | loss_G_cycle: 0.9783 | loss_D: 0.2959 -- ETA: 5:46:43.120403\n",
      "Epoch 074/200 [0100/0100] -- loss_G: 2.6063 | loss_G_identity: 0.5197 | loss_G_GAN: 1.0759 | loss_G_cycle: 1.0107 | loss_D: 0.2816 -- ETA: 5:43:58.575792\n",
      "Epoch 076/200 [0100/0100] -- loss_G: 2.4783 | loss_G_identity: 0.4851 | loss_G_GAN: 1.0613 | loss_G_cycle: 0.9318 | loss_D: 0.2799 -- ETA: 5:38:29.489215\n",
      "Epoch 077/200 [0100/0100] -- loss_G: 2.5137 | loss_G_identity: 0.4997 | loss_G_GAN: 1.0711 | loss_G_cycle: 0.9429 | loss_D: 0.2742 -- ETA: 5:35:45.132221\n",
      "Epoch 078/200 [0100/0100] -- loss_G: 2.5824 | loss_G_identity: 0.5171 | loss_G_GAN: 1.0767 | loss_G_cycle: 0.9886 | loss_D: 0.2835 -- ETA: 5:33:00.880095\n",
      "Epoch 079/200 [0100/0100] -- loss_G: 2.5536 | loss_G_identity: 0.5016 | loss_G_GAN: 1.0556 | loss_G_cycle: 0.9964 | loss_D: 0.2689 -- ETA: 5:30:16.586604\n",
      "Epoch 080/200 [0100/0100] -- loss_G: 2.4582 | loss_G_identity: 0.4750 | loss_G_GAN: 1.0845 | loss_G_cycle: 0.8987 | loss_D: 0.2765 -- ETA: 5:27:31.815468\n",
      "Epoch 081/200 [0100/0100] -- loss_G: 2.4693 | loss_G_identity: 0.4728 | loss_G_GAN: 1.1057 | loss_G_cycle: 0.8907 | loss_D: 0.2741 -- ETA: 5:24:47.304404\n",
      "Epoch 082/200 [0100/0100] -- loss_G: 2.5191 | loss_G_identity: 0.4717 | loss_G_GAN: 1.1474 | loss_G_cycle: 0.9000 | loss_D: 0.2321 -- ETA: 5:22:02.826620\n",
      "Epoch 083/200 [0100/0100] -- loss_G: 2.5919 | loss_G_identity: 0.4909 | loss_G_GAN: 1.1605 | loss_G_cycle: 0.9405 | loss_D: 0.2501 -- ETA: 5:19:18.525191\n",
      "Epoch 084/200 [0100/0100] -- loss_G: 2.6165 | loss_G_identity: 0.4907 | loss_G_GAN: 1.2085 | loss_G_cycle: 0.9173 | loss_D: 0.2303 -- ETA: 5:16:34.081920\n",
      "Epoch 085/200 [0100/0100] -- loss_G: 2.6549 | loss_G_identity: 0.4943 | loss_G_GAN: 1.2267 | loss_G_cycle: 0.9339 | loss_D: 0.2043 -- ETA: 5:13:49.815332\n",
      "Epoch 086/200 [0100/0100] -- loss_G: 2.6851 | loss_G_identity: 0.4964 | loss_G_GAN: 1.2552 | loss_G_cycle: 0.9334 | loss_D: 0.2204 -- ETA: 5:11:05.702292\n",
      "Epoch 087/200 [0100/0100] -- loss_G: 2.5646 | loss_G_identity: 0.4661 | loss_G_GAN: 1.2231 | loss_G_cycle: 0.8755 | loss_D: 0.2084 -- ETA: 5:08:21.268895\n",
      "Epoch 088/200 [0100/0100] -- loss_G: 2.6734 | loss_G_identity: 0.4966 | loss_G_GAN: 1.2312 | loss_G_cycle: 0.9455 | loss_D: 0.2092 -- ETA: 5:05:37.183790\n",
      "Epoch 089/200 [0100/0100] -- loss_G: 2.6821 | loss_G_identity: 0.4843 | loss_G_GAN: 1.2801 | loss_G_cycle: 0.9177 | loss_D: 0.2014 -- ETA: 5:02:53.086009\n",
      "Epoch 090/200 [0100/0100] -- loss_G: 2.6385 | loss_G_identity: 0.4856 | loss_G_GAN: 1.2535 | loss_G_cycle: 0.8994 | loss_D: 0.2032 -- ETA: 5:00:08.972242\n",
      "Epoch 091/200 [0100/0100] -- loss_G: 2.6322 | loss_G_identity: 0.4745 | loss_G_GAN: 1.2865 | loss_G_cycle: 0.8713 | loss_D: 0.1920 -- ETA: 4:57:25.018068\n",
      "Epoch 092/200 [0100/0100] -- loss_G: 2.6104 | loss_G_identity: 0.4669 | loss_G_GAN: 1.2963 | loss_G_cycle: 0.8472 | loss_D: 0.1979 -- ETA: 4:54:41.267461\n",
      "Epoch 093/200 [0100/0100] -- loss_G: 2.5406 | loss_G_identity: 0.4646 | loss_G_GAN: 1.2227 | loss_G_cycle: 0.8533 | loss_D: 0.2712 -- ETA: 4:51:57.228372\n",
      "Epoch 094/200 [0100/0100] -- loss_G: 2.3587 | loss_G_identity: 0.4569 | loss_G_GAN: 1.0384 | loss_G_cycle: 0.8634 | loss_D: 0.3103 -- ETA: 4:49:13.034464\n",
      "Epoch 095/200 [0100/0100] -- loss_G: 2.5687 | loss_G_identity: 0.4807 | loss_G_GAN: 1.2036 | loss_G_cycle: 0.8844 | loss_D: 0.2224 -- ETA: 4:46:29.071808\n",
      "Epoch 096/200 [0100/0100] -- loss_G: 2.6362 | loss_G_identity: 0.4766 | loss_G_GAN: 1.2841 | loss_G_cycle: 0.8755 | loss_D: 0.1944 -- ETA: 4:43:45.101485\n",
      "Epoch 097/200 [0100/0100] -- loss_G: 2.7148 | loss_G_identity: 0.4727 | loss_G_GAN: 1.3404 | loss_G_cycle: 0.9017 | loss_D: 0.1785 -- ETA: 4:41:01.031007\n",
      "Epoch 098/200 [0100/0100] -- loss_G: 2.6822 | loss_G_identity: 0.4781 | loss_G_GAN: 1.3265 | loss_G_cycle: 0.8777 | loss_D: 0.2043 -- ETA: 4:38:17.132645\n",
      "Epoch 100/200 [0100/0100] -- loss_G: 2.4128 | loss_G_identity: 0.4584 | loss_G_GAN: 1.1048 | loss_G_cycle: 0.8496 | loss_D: 0.2763 -- ETA: 4:32:49.179964\n",
      "Epoch 102/200 [0100/0100] -- loss_G: 2.6245 | loss_G_identity: 0.4542 | loss_G_GAN: 1.3358 | loss_G_cycle: 0.8345 | loss_D: 0.1712 -- ETA: 4:27:21.257088\n",
      "Epoch 103/200 [0100/0100] -- loss_G: 2.5561 | loss_G_identity: 0.4553 | loss_G_GAN: 1.2673 | loss_G_cycle: 0.8336 | loss_D: 0.2185 -- ETA: 4:24:37.297429\n",
      "Epoch 104/200 [0100/0100] -- loss_G: 2.5570 | loss_G_identity: 0.4582 | loss_G_GAN: 1.2814 | loss_G_cycle: 0.8174 | loss_D: 0.2052 -- ETA: 4:21:53.459367\n",
      "Epoch 105/200 [0100/0100] -- loss_G: 2.6563 | loss_G_identity: 0.4574 | loss_G_GAN: 1.3727 | loss_G_cycle: 0.8262 | loss_D: 0.1788 -- ETA: 4:19:09.438532\n",
      "Epoch 106/200 [0100/0100] -- loss_G: 2.5635 | loss_G_identity: 0.4447 | loss_G_GAN: 1.3227 | loss_G_cycle: 0.7961 | loss_D: 0.1694 -- ETA: 4:16:25.388895\n",
      "Epoch 107/200 [0100/0100] -- loss_G: 2.5382 | loss_G_identity: 0.4403 | loss_G_GAN: 1.3225 | loss_G_cycle: 0.7754 | loss_D: 0.1900 -- ETA: 4:13:41.556870\n",
      "Epoch 108/200 [0100/0100] -- loss_G: 2.5694 | loss_G_identity: 0.4426 | loss_G_GAN: 1.3451 | loss_G_cycle: 0.7817 | loss_D: 0.1870 -- ETA: 4:10:57.819946\n",
      "Epoch 109/200 [0100/0100] -- loss_G: 2.5931 | loss_G_identity: 0.4439 | loss_G_GAN: 1.3522 | loss_G_cycle: 0.7970 | loss_D: 0.1774 -- ETA: 4:08:13.940473\n",
      "Epoch 110/200 [0100/0100] -- loss_G: 2.5785 | loss_G_identity: 0.4477 | loss_G_GAN: 1.3400 | loss_G_cycle: 0.7908 | loss_D: 0.1772 -- ETA: 4:05:30.128312\n",
      "Epoch 111/200 [0100/0100] -- loss_G: 2.5621 | loss_G_identity: 0.4345 | loss_G_GAN: 1.3631 | loss_G_cycle: 0.7645 | loss_D: 0.1782 -- ETA: 4:02:46.242567\n",
      "Epoch 112/200 [0100/0100] -- loss_G: 2.6299 | loss_G_identity: 0.4467 | loss_G_GAN: 1.3848 | loss_G_cycle: 0.7984 | loss_D: 0.1787 -- ETA: 4:00:02.431420\n",
      "Epoch 113/200 [0100/0100] -- loss_G: 2.8777 | loss_G_identity: 0.4708 | loss_G_GAN: 1.5280 | loss_G_cycle: 0.8790 | loss_D: 0.1465 -- ETA: 3:57:18.656802\n",
      "Epoch 114/200 [0100/0100] -- loss_G: 2.7333 | loss_G_identity: 0.4459 | loss_G_GAN: 1.4567 | loss_G_cycle: 0.8307 | loss_D: 0.1324 -- ETA: 3:54:34.692082\n",
      "Epoch 115/200 [0100/0100] -- loss_G: 2.6606 | loss_G_identity: 0.4530 | loss_G_GAN: 1.4014 | loss_G_cycle: 0.8062 | loss_D: 0.1623 -- ETA: 3:51:50.909126\n",
      "Epoch 116/200 [0100/0100] -- loss_G: 2.5940 | loss_G_identity: 0.4408 | loss_G_GAN: 1.3825 | loss_G_cycle: 0.7708 | loss_D: 0.1609 -- ETA: 3:49:06.942517\n",
      "Epoch 117/200 [0100/0100] -- loss_G: 2.5362 | loss_G_identity: 0.4357 | loss_G_GAN: 1.3507 | loss_G_cycle: 0.7498 | loss_D: 0.1904 -- ETA: 3:46:23.150133\n",
      "Epoch 118/200 [0100/0100] -- loss_G: 2.5659 | loss_G_identity: 0.4374 | loss_G_GAN: 1.3669 | loss_G_cycle: 0.7616 | loss_D: 0.1667 -- ETA: 3:43:39.220038\n",
      "Epoch 119/200 [0100/0100] -- loss_G: 2.5615 | loss_G_identity: 0.4229 | loss_G_GAN: 1.3921 | loss_G_cycle: 0.7465 | loss_D: 0.1645 -- ETA: 3:40:55.332258\n",
      "Epoch 120/200 [0100/0100] -- loss_G: 2.5015 | loss_G_identity: 0.4259 | loss_G_GAN: 1.3440 | loss_G_cycle: 0.7316 | loss_D: 0.1750 -- ETA: 3:38:11.529312\n",
      "Epoch 121/200 [0100/0100] -- loss_G: 2.5214 | loss_G_identity: 0.4279 | loss_G_GAN: 1.3642 | loss_G_cycle: 0.7293 | loss_D: 0.1662 -- ETA: 3:35:27.737363\n",
      "Epoch 122/200 [0100/0100] -- loss_G: 2.5122 | loss_G_identity: 0.4241 | loss_G_GAN: 1.3540 | loss_G_cycle: 0.7340 | loss_D: 0.1711 -- ETA: 3:32:43.946341\n",
      "Epoch 123/200 [0100/0100] -- loss_G: 2.4948 | loss_G_identity: 0.4169 | loss_G_GAN: 1.3568 | loss_G_cycle: 0.7212 | loss_D: 0.1721 -- ETA: 3:30:00.059772\n",
      "Epoch 124/200 [0091/0100] -- loss_G: 2.4561 | loss_G_identity: 0.4150 | loss_G_GAN: 1.3368 | loss_G_cycle: 0.7042 | loss_D: 0.2294 -- ETA: 3:27:31.106284"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Set model input\n",
    "        real_A = Variable(input_A.copy_(batch['A']))\n",
    "        real_B = Variable(input_B.copy_(batch['B']))\n",
    "\n",
    "        ###### Generators A2B and B2A ######\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        # G_A2B(B) should equal B if real B is fed\n",
    "        same_B = netG_A2B(real_B)\n",
    "        loss_identity_B = criterion_identity(same_B, real_B)*5.0\n",
    "        # G_B2A(A) should equal A if real A is fed\n",
    "        same_A = netG_B2A(real_A)\n",
    "        loss_identity_A = criterion_identity(same_A, real_A)*5.0\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = netG_A2B(real_A)\n",
    "        pred_fake = netD_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, target_real)\n",
    "\n",
    "        fake_A = netG_B2A(real_B)\n",
    "        pred_fake = netD_A(fake_A)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake, target_real)\n",
    "\n",
    "        # Cycle loss\n",
    "        recovered_A = netG_B2A(fake_B)\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A)*10.0\n",
    "\n",
    "        recovered_B = netG_A2B(fake_A)\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B)*10.0\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "        loss_G.backward()\n",
    "        \n",
    "        optimizer_G.step()\n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator A ######\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = netD_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        fake_A = fake_A_buffer.push_and_pop(fake_A)\n",
    "        pred_fake = netD_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_A.backward()\n",
    "\n",
    "        optimizer_D_A.step()\n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator B ######\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = netD_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "        \n",
    "        # Fake loss\n",
    "        fake_B = fake_B_buffer.push_and_pop(fake_B)\n",
    "        pred_fake = netD_B(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_B.backward()\n",
    "\n",
    "        optimizer_D_B.step()\n",
    "        ###################################\n",
    "\n",
    "        # Progress report (http://localhost:8097)\n",
    "        logger.log({'loss_G': loss_G, 'loss_G_identity': (loss_identity_A + loss_identity_B), 'loss_G_GAN': (loss_GAN_A2B + loss_GAN_B2A),\n",
    "                    'loss_G_cycle': (loss_cycle_ABA + loss_cycle_BAB), 'loss_D': (loss_D_A + loss_D_B)}, \n",
    "                    images={'real_A': real_A, 'real_B': real_B, 'fake_A': fake_A, 'fake_B': fake_B})\n",
    "    \n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    # Save models checkpoints\n",
    "    torch.save(netG_A2B.state_dict(), 'output_PPG/netG_A2B.pth')\n",
    "    torch.save(netG_B2A.state_dict(), 'output_PPG/netG_B2A.pth')\n",
    "    torch.save(netD_A.state_dict(), 'output_PPG/netD_A.pth')\n",
    "    torch.save(netD_B.state_dict(), 'output_PPG/netD_B.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15c2fe-5814-4a3e-a1bd-3ddf49ab3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(optimizer_G.state_dict(), 'output_PPG/optimizer_G.pth')\n",
    "torch.save(optimizer_D_A.state_dict(), 'output_PPG/optimizer_D_A.pth')\n",
    "torch.save(optimizer_D_B.state_dict(), 'output_PPG/optimizer_D_B.pth')\n",
    "\n",
    "#learningrateschedulers\n",
    "\n",
    "torch.save(lr_scheduler_G.state_dict(), 'output_PPG/lr_scheduler_G.pth')\n",
    "torch.save(lr_scheduler_D_A.state_dict(), 'output_PPG/lr_scheduler_D_A.pth')\n",
    "torch.save(lr_scheduler_D_B.state_dict(), 'output_PPG/lr_scheduler_D_B.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4032d-9e74-45aa-90c9-375186851732",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G.load_state_dict(torch.load('output_PPG/optimizer_G.pth'))\n",
    "optimizer_D_A.load_state_dict(torch.load('output_PPG/optimizer_D_A.pth'))\n",
    "optimizer_D_B.load_state_dict(torch.load('output_PPG/optimizer_D_B.pth'))\n",
    "\n",
    "#learningrateschedulers\n",
    "\n",
    "lr_scheduler_G.load_state_dict(torch.load('output_PPG/lr_scheduler_G.pth'))\n",
    "lr_scheduler_D_A.load_state_dict(torch.load('output_PPG/lr_scheduler_D_A.pth'))\n",
    "lr_scheduler_D_B.load_state_dict(torch.load('output_PPG/lr_scheduler_D_B.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a12d4-9078-4a00-af8d-65fde496851c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
